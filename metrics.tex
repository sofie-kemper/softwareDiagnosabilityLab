\documentclass{scrartcl}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{csquotes}
\usepackage{hyperref}
\usepackage{todonotes}

\begin{document}

\title{Design and Software Quality Metrics}
\subtitle{Exploring the Relationship between Design Metrics and Software
Diagnosability using Machine Learning (2018)}
\author{Thomas Dornberger \and Sofie Kemper}
\date{2018-05-17}

\maketitle

\section{Metrics}

\subsection{Static Metrics}

The following metrics are \emph{static} metrics, i.e., they are calculated based
on the source code. All metrics that might be correlated to the size of the
project's code base are normalised by the number of the lines of source code in
the project. This is indicated by the postfix \enquote{-D}, e.g., F-ML-D.

\subsubsection{Size and Complexity Information}

In cases where size and complexity metrics can only be measured at the method or
file level, we aggregate the calculated values such that we obtain the maximum,
average and median values for the whole project.

\paragraph{Lines of Code including Comments (LOC)}

The \emph{number of lines of code} qantifies a component's size by counting the
number of lines of code excluding blank lines. Comments are included in this
measure. We use TeamScale (see \ref{sec:teamscale}) to measure the number of
lines of code.

\paragraph{Lines of Code without Comments (SLOC)}

The \emph{number of lines of code without comments} or \emph{source lines of
code} measures the size of actual
source code of a component, i.e., it quantifies the size of the source code
excluding blank lines and comments. We measure it using Lizard (see
\ref{sec:lizard}) as well as TeamScale (see \ref{sec:teamscale}).

\paragraph{High Length Methods (F-ML)}

This metric quantifies the occurrence of methods that are of medium to high
length, i.e., methods with more than 30 SLOC. It is provided by TeamScale (see
\ref{sec:teamscale}).

In addition, we measure the proportion of high-length methods (F-PML).

\paragraph{Files of High Size (F-HFS)}

This metric quantifies the occurrenc of very long files, i.e., files containing
more than 750 SLOC. It is extracted using TeamScale (see \ref{sec:teamscale}).

In addition, we calculate the proportion of files of high size (F-PHFS).

\paragraph{Files of Medium Size (F-MFS)}

Using TeamScale (see \ref{sec:teamscale}), we obtain this number of medium to
long files, i.e., files containing between 300 and 750 SLOC.

In addition, we measure the proportion of files of medium size (F-PMFS).

\paragraph{Methods with High Nesting Depth (F-HND)}

The nesting depth is an indicator for the complexity of a method. It measures
the number of statement blocks nested due to the use of control structures. This
metric quantifies the occurrence of methods with a very high nesting depth,
i.e., a nesting depth higher than 5. It is provided by TeamScale (see
\ref{sec:teamscale}).

In addition, we use the proportion of methods with high nesting depth (F-PHND).

\paragraph{Methods with Medium Nesting Depth (F-MND)}

This metric indicates the number of methods with medium nesting depth, i.e.,
methods with a corresponding nesting depth between 3 and 5. It is provided by
TeamScale (see \ref{sec:teamscale}).

In addition to this absolute number, we calculate the proportion of methods with
medium nesting depth (F-PMND).

\paragraph{Token Count of Functions(TCF)}

The \emph{token count of a function} describes the number of conditional
statement tokens in a function. It is used to calculate the cyclomatic
complexity number. We use the tool Lizard (see \ref{sec:lizard}) to measure this
function-level property and aggregate the obtained values as described above.

\paragraph{Parameter Count of Functions (PCF)}

The \emph{parameter count of a function} quanitifies the number of parameters a
given function takes. We measure this function-level property using Lizard (see
\ref{sec:lizard}) and aggregate the obtained values as described above.

\paragraph{Cyclomatic Complexity Number (CCN)}

The \emph{cyclomatic complexity} or \emph{McCabe's complexity} of a component
describes its perceived complexity. It is proportionate to the number of
linearly independent paths through a program, i.e., if-statements
or while-loops increase this value. A high cyclomatic complexity number
indicates that code is hard to read, understand, and maintain which might lead
to more bugs. We measure the cyclomatic complexity on a
method-level using Lizard (see \ref{sec:lizard}) and aggregate the values as
described above. In addition, we also use the cyclomatic complexity numbers
(aggregated project-level data) provided by TeamScale (see \ref{sec:teamscale}).

\paragraph{Maximum Cyclomatic Complexity (MAXCC)}

The maximum cyclomatic complexity quantifies the complexity of the most complex
file in the analysed source code.

\paragraph{Number of High Cyclomatic Complexity Methods (HCC)}

This metric counts the number of methods which are scored as highly complex,
i.e., those with a cyclomatic complexity number greater than 20. We calculate it
using TeamScale (see \ref{sec:teamscale}).

\paragraph{Number of Medium Cyclomatic Complexity Methods (MCC)}

This metric represents the number of methods of medium complexity, i.e., those
with a cyclomatic complexity number between 10 and 20 ($CC \in (10, 20]$). It is
provided by TeamScale (see \ref{sec:teamscale}).

\paragraph{Number of Low Cyclomatic Complexity Methods (LCC)}

This metric is used to measure the number of methods of low complexity. Low
complexity methods are those that show a cyclomatic complexity number smaller
than 10. TeamScale (see \ref{sec:teamscale}) is used to obtain this metric.

\paragraph{Proportion of High/Medium/Low Complexity Methods (PHCC/PMCC/PLCC)}

We define the proportions of high (respectively medium or low) complexity
methods by using the absolute number of high (respectively medium or low)
complexity methods (see above)  provided by TeamScale (see \ref{sec:teamscale})
and normalising it by the total number of methods measured, thus, obtaining a
relative occurrence of highlyy (respectively moderately or lowly) complex
methods.

\subsubsection{Coupling and Cohesion Metrics}

The following metrics try to capture coupling and cohesion of the source code.
They are all provided by the tool JPeek (see \ref{sec:jpeek}). we use the
aggregated metrics provided by this tool where applicable: minimum value,
maximum value, number of classes scored as \enquote{Green}, \enquote{Yellow},
and \enquote{Red}, respectively. In addition, the total score and the percentage
of defects are used for each of the 5 metrics.

\paragraph{Cohesion Among Method of Class (CAMC)}

This metric measures the cohesion among the methods of a given class. Formally,
it indicates the extent of intersections of individual method parameter type
lists with the list of parameter types in all methods in the class.

\paragraph{Lack of Cohesion of Methods (LCOM5)}

This metric was proposed by Henderson and Sellers and can be interpreted as the
number of pairs of methods in a given class having no common attribute. Hence,
it is based on method similarity among methods of a class. It provides a measure
of class cohesion expressed as percentage value ($\in [0,1]$). A value of 0
indicates full cohesion while a value of 1 indicates no cohesion.

\paragraph{Method-Method through Attributes Cohesion (MMAC)}

This metric is an indicator of method-method cohesion. The similarity between
a pair of methods is expressed as a function of their shared properties. The
metric defines the average cohesion of all pairs of methods.

\paragraph{Normalised Hamming Distance (NHD)}

This metric measures class-level cohesion uses the similarity of parameter types
of the class's methods as basis for measuring cohesion. It compares pairs of
methods, counting a disagreement only if a parameter type is used by one of the
methods and not used by the other method. It was developed as an alternative to
CAMC in order to prevent false positives and have a measure with a finer
granularity. 

\paragraph{Sensitive Class Cohesion Metric (SCOM)}

This metric measures cohesion via the proportion of class attributes used in the
class's methods. It yields values in the range $[0,1]$ where 0 indicates total
lack of cohesion, i.e., every method deals with an independent set of
attributes, and 1 indicates full cohesion, i.e., all class attributes are used
by every single method.

\subsubsection{FindBugs-Findings}

All following findings are provided by FindBugs which is integrated in TeamScale
(see \ref{sec:teamscale}).

\paragraph{Performance Code Smell Findings (FB-P)}

This metric counts the number of found code smells regarding performance.
Examples include the boing and subsequent unboxing of a value or the invocation
of \texttt{.toString()} on a String.

\paragraph{Malicious Code Vulnerability Findings (FB-MCV)}

\emph{Malicious code vulnerabilities} quantifies the number of of code smells in
this category, e.g., a \texttt{finalize} method that is \texttt{public} instead
of \texttt{protected}.

\paragraph{Security Findings (FB-SEC)}

This metric measure the number of finding in the code smell category security,
e.g., a JSP-reflected cross-site scripting vulnerability.

\paragraph{Dodgy Code Findings (FB-DC)}

This metric quantifies the occurrence of \enquote{dodgy code}, e.g.,
unchecked/unconfirmed casts.

\paragraph{Correctness Findings (FB-COR)}

\emph{Correctness findings} counts the number of code smell findings in the
category of correctness, e.g., impossible downcasts.

\paragraph{Multithreaded Correctness Findings (FB-MCOR)}

This metric measures the number of findings regarding multithreaded correctness,
e.g., attribute with a \texttt{get}-method that is not \texttt{synchronized}
while the corresponding \texttt{set}-method is \texttt{synchronized}.

\paragraph{Bad Practice Findings (FB-BP)}

This metric counts the number of bad practice findings, e.g., using a rough
value of a known constant.

\subsubsection{TeamScale-Findings}

All following metrics are obtained via the tool TeamScale (see
\ref{sec:teamscale})

\paragraph{Number of TeamScale-Findings (CF)}

This metric provides the total, aggregated number of findings by the tool
TeamScale (see \ref{sec:teamscale}), i.e., including TeamScale's own as well as
FindBug's code findings. All the findings listed below as well as the findings
regarding structure (F-HML, F-MML, F-HFS, F-MFS, F-HND, F-MND) are contained in
this number.

\paragraph{Missing Braces for Block Statements Findings (F-MBB)}

This metric quantifies the occurrence of the code anomaly that block statements
miss braces.

\paragraph{Null Return Optional Findings (F-RT)}

This metric counts how often a method returns \texttt{null} although the return
type is \texttt{Optional}.

\paragraph{Missing Code Findings (F-MC)}

\emph{Missing code findings} quantifies how often empty blocks, code that is
\enquote{commented out} or files which don't contain any code occur.

\paragraph{Test Convention Findings (F-TC)}

This metric counts how often test conventions are violated. This includes the
naming of test classes as well as the usage of \texttt{@ignore} and inverted
conditions.

\paragraph{Null Pointer Dereference Findings (F-NP)}

This metric quantifies the occurrence of possible \texttt{null} pointer
dereferences at runtime due to wrong \texttt{null} assignments or missing checks
before dereferencing.

\paragraph{Unused Variable/Parameter Findings (F-UVP)}

This metric counts the number of unused variables or parameters.

\paragraph{Exception Handling Findings (F-EH)}

\emph{Exception Handling Findings} represents the number of code smell findings
regarding exception handling, e.g., catching or throwing generic exceptions or
the loss of the stacktrace.

\paragraph{Performance: \texttt{Contains} on List Findings (F-PCL)}

This metric quantifies how often \texttt{contains()} is called on a list, which
is a performance issue.

\paragraph{Bad Practice Findings (F-BP)}

The category of bad practice code smells includes star imports or methods with
the same name as methods in \texttt{Object}. They supplement the bad practice
findings provided by FindBugs. This metric quantifies the occurrence of such
findings.

\paragraph{Unused Code Findings (F-UC)}

This metric quantifies the occurrence of unused \texttt{private} fields or
methods.

\paragraph{Code Formatting Findings (F-CF)}

This metric counts how often problems regarding the code formatting are found.
This includes the problems of multiple statements or declarations in the same
line.

\paragraph{Cloning Findings (F-CL)}

The \emph{cloning findings} metric counts the number of clones, i.e., duplicated
code. A high number of clones can cause problems regarding the code maintenance
and, thus, introduce bugs.

\paragraph{Clone Coverage (F-CLC)}

The \emph{clone coverage} describes how likely it is that a random SLOC is
cloned at another position as percentage.

\subsection{Dynamic Metrics}

The following metrics are \emph{dynamic} design metrics, i.e., they are
calculated based on a program's call or data dependency graph as generated by
the tool JDCallgraph (see \ref{sec:jdcallgraph}). All metrics are computed on
the data dependency graph (prefix DD-) as well as the callgraph (prefix CG-)
using the statistical programming language R and the igraph library (see
\ref{sec:igraph}). We use the multi-edge versions of these graphs and simplify
them where appropriate (see below).

\subsubsection{Vertex Count (DD-VC, CG-VC)}

The vertex count is a measure of graph size. It quantifies the number of
vertices in the graph.

\subsubsection{Edge Count (DD-EC, CG-EC)}

The edge count is a measure of graph size, indicating the number of edges
(including multi-edges) between pairs of vertices of the graph.

\subsubsection{Simplified Edge Count (DD-SEC, CG-SEC)}

The simplified edge count corresponds to the number of edges in the graph, not
counting multi-edges, i.e., if there are multiple edges between one pair of
vertices, only one of these edges is counted.

\subsubsection{Multi-Edge Proportion (DD-MEP, CG-MEP)}

This metric indicates the proportion of multi-edges in the graph, i.e., which
fraction of the edges present in the graph are actually multi-edges.

\subsubsection{Maximum Vertex Degree (DD-MAXVD, CG-MAXVD)}

The maximum vertex degree is the maximum total degree of any vertex in the
graph.

\subsubsection{Mean Vertex Degree (DD-MVD, CG-MVD)}

The mean vertex degree is the average of all vertex total degrees in the graph.

\subsubsection{Vertex Degree Quantiles (DD-VD*Q, CG-VD*Q)}

We look at the vertex degree distribution by aggregating the data in statistical
quantiles. The 90-th, 80-th, 75-th and 50-th (median) quantiles are used. For
instance, the 90-th quantile (denoted by CG-VD90Q) indicates the threshold
below which 90\% of all vertex degrees lie.

\subsubsection{Maximum Vertex In-Degree (DD-MAXVI, CG-MAXVI)}

The maximum vertex in-degree is the maximum in-degree of any vertex in the
graph, where in-degrees are the number of directed edges with the given vertex
as end point.

\subsubsection{Mean Vertex In-Degree (DD-MVI, CG-MVI)}

This metric is calculated as the average of all vertices' in-degrees.

\subsubsection{Maximum Vertex Out-Degree (DD-MAXVO, CG-MAXVO)}

The maximum vertex out-degree indicates the maximum number of outgoing edges for
all vertices in the graph.

\subsubsection{Mean Vertex Out-Degree (DD-MVO, CG-MVO)}

This metric denotes the average of all vertices' out-degrees.

\subsubsection{Mean Start-Node Degree (DD-MSND, CG-MSND)}

This metric denotes the average degree of the start nodes. Start nodes are the
test cases, i.e., those nodes in the graph which possess only outgoing edges. It
indicates how many methods are called directly by the test cases.

\subsubsection{Graph Diameter (DD-GD, CG-GD)}

The graph diameter is a measure of a graph's size, indicating the geatest
distance between any pair of vertices.

\subsubsection{Graph Radius (DD-GR, CG-GR)}

The graph radius denotes the minimum eccentricity of any vertex in the graph.
The eccentricity of a vertex indicates how far this vertex is from that vertex
most distant from it in the graph.

\subsubsection{Mean Distance (DD-MD, CG-MD)}

This metric indicates the mean (geodesic) distance between all pairs of nodes in
the graph.

\subsubsection{Maximum Eigenvector Centrality (DD-MEC, CG-MEC)}

A vertex' eigenvector centrality indicates its importance or influence in the
graph, e.g., nodes that are connected to many nodes, especially to many other
important nodes, show a high eigencentrality. Each vertex has a relative score.
Google's PageRank is a randomised version of eigenvector centrality which is
more robust. 

This metric denotes the maximum eigencentrality found in the graph.

\subsubsection{Eigenvector Centrality Quantiles (DD-EC*Q, CG-EC*Q)}

This metric characterises the distribution of vertices' eigenvector centrality
scores by providing the 90-th, 80-th, 75-th, and 50-th quantiles (see above) of
all the eigencentrality scores in the graph.

\subsubsection{Vertex Connectivity (DD-VCON, CG-VCON)}

The vertex connectivity is a metric that indicates how many vertices need to be
removed to render the graph disconnected, i.e., ensure that every vertex can no
longer reach every other vertex. It might indicate how dense the graph, and
thus, how complex its call- or data-dependency-structure, is.

\subsubsection{Edge Connectivity (DD-ECON, CG-ECON)}

A graph's edge connectivity indicates how many edges need to be removed from the
graph to create a disconnected graph.

\subsubsection{(Average) Clustering Coefficient (DD-CC, CG-CC)}

A vertex' clustering coefficient captures the local connectivity, i.e., it
measure how likely it is that any two of its neighbour's are connected amongst
each other. This score is aggregated via averaging over all vertices in the
graph to get a clustering coefficient for the entire graph, which indicates the
extent to which clusters exist in the graph.

\subsection{Test Suite Characteristics}

The following metrics aim to quantitatiely analyse a project's test suite. We
use basic metrics provided via Gzoltar (see \ref{sec:gzoltar}) as well as test
suite metrics proposed by Perez et al in \enquote{A Test-suite Diagnosability
Metric for Spectrum-based Fault Localization Approaches} and shown to be good
indicators of spectrum-based fault localisation accuracy. These metrics are
computed based on the coverage data provided by Gzoltar (see \ref{sec:gzoltar}).

\subsubsection{Number of Passed Test Cases (T-NP)}

The total number of test cases out of the test suite that were passed, i.e.,
executed successfully. We measure this based on all test classes, not solely
the ones that fail for the current buggy version. This metric is calculated via
Gzoltar (see \ref{sec:gzoltar}).

\subsubsection{Proportion of Passed Test Cases (T-PP)}

The proportion of passed test cases out of all test cases.

\subsubsection{Number of Failed Test Cases (T-NF)}

The total number of test cases out of the test suite that were failed, i.e.,
that could not be executed successfully. This number can be extracted by Gzoltar
(see \ref{sec:gzoltar}) or accessed via
\url{http://program-repair.org/defects4j-dissection/#!/}.

\subsubsection{Proportion of Failed Test Cases (T-PF)}

The proportion of failed test cases out of all test cases. This number can be
extracted by Gzoltar (see \ref{sec:gzoltar}) or accessed via
\url{http://program-repair.org/defects4j-dissection/#!/}.

\subsubsection{Number of Test Cases (T-N)}
\todo{Use?}
The total number of test cases in the test suite. This number can be accessed
via Gzoltar (see \ref{sec:gzoltar}).

\subsubsection{Number of Relevant Test Cases (T-RN)}

The number of test cases that are relevant for the buggy version, i.e., the
number of test cases in the classes in which at least one test case fails
because of the bug. This measure is provided by gzoltar (see \ref{sec:gzoltar}).

\subsubsection{Test Density (T-D)}

The test density is a measure that ensures that components are frequently
involved in tests. It is computed as T-D$=\sum{A[i,j]}/(N\cdot M)$ and then
normalised, s.t., a value of $1.0$ is the target.

\subsubsection{Test Diversity (T-G)}

The test diversity tries to quantify to what extent components are tested in
diverse combinations. We compute it as T-G$=1 - \sum{n \cdot (n-1)}/ (N \cdot
(N-1))$ which yields values in the interval $[0;1]$ where $1$ is seen as the
optimal value.

\subsubsection{Test Uniqueness (T-U)}

The test uniqueness tries to measure to what extent components are
distinguishable based on the test results. It is computed as T-U=$|\textrm{T-G}|
/ \textrm{T-M}$. 

\subsubsection{Perez et al's diagnostic predictor (T-DDU)}

This metric tries to combine the previous ones in a way that provides the
maximum amount of information concerning the test suite's influence on software
diagnosability. It is partly based on the notion of entropy. The metric is
computed via the formula $\textrm{T-DDU} = \textrm{T-P} \cdot \textrm{T-G} \cdot
\textrm{T-U}$ and, thus, produces values in the interval $[0;1]$ where a value
of $1$ is regarded as ideal.

\subsection{Bug Characteristics}

The following metrics are related to a known bug and aim to quantify features of
the bug such as its location and size.

\section{Tools}

We have tried a multitude of different tools (SourceMeter, LOCC, SonarQube,
CCCC, USC CodeCount, CLOC, etc.) and chosen the following for the interesting
metrics they provide as well as the possibility to automate their usage, i.e.,
the possibility to execute them on the command line and output formats that can
be easily parsed and used for our purposes.

\subsection{Lizard}
\label{sec:lizard}

Lizard is a Python-based tool that analyses code size and perceived code
complexity as well as parameter and token counts at the function level. It can
analyse Java, C/C++, JavaScript, Python, Ruby, Swift, PHP, Scala, and Objective
C scripts.  More information can be found at
\url{https://pypi.org/project/lizard/}.

\subsection{TeamScale}
\label{sec:teamscale}

TeamScale is a tool that provides a multitude of static code analyses to
indicate code quality and possible quality defects (\enquote{findings}). It is
free for academic usage. The tool monitors the quality of code over time by
using manualyy checked-in versions or automatically detected versions of a given
repository. By using the diffs between versions, it can quickly analyse the
history of a project. The tool FindBugs (see
\url{http://findbugs.sourceforge.net}) is also contained in TeamScale. The
metrics are calculated on the file or even method level and aggregated
hierarchically. Hence, we can obtain project metrics easily.

More information regarding TeamScale can be found at
\url{https://www.cqse.eu/en/products/teamscale/landing/}.

We have implemented a REST-client to automatically query the metric values from
TeamScale and to transform them into a format we can easily use for our
analysis.

\subsection{JPeek}
\label{sec:jpeek}

JPeek is a static collector of Java code metrics relating to cohesion and
coupling. It measures 5 metrics on the method level and aggregates this data
quantitatively, e.g., calculating min, max, variance, and other statistical
measures,  as well as qualitatively, i.e., \enquote{scoring} components as a
whole and categorising them into the three classes \enquote{green},
\enquote{yellow}, and \enquote{red}. In addition, it defines and measures
\enquote{defects} which are classes whose scores particularly bad using the mean
scores as baseline.  

We use the version JPeek 0.26.3. For more information on the tool, the
interested reader is referred to \url{http://www.jpeek.org} and
\url{https://github.com/yegor256/jpeek}.

\subsubsection{JDCallgraph}
\label{sec:jdcallgraph}

JDCallgraph is a tool for dynamic call graph generation for Java. Call graphs as
well as data dependency graphs can be computed and are provided in .dot format.
In addition, JDCallgraph can provide coverage and other information which we do
not use in this project. More details can be found via
\url{https://github.com/dkarv/jdcallgraph}.

The .dot-files are read into R (see \ref{sec:igraph}) via the R library sna (see
\url{https://www.rdocumentation.org/packages/sna)}.

\subsection{R igraph Library}
\label{sec:igraph}

igraph is a library for network analysis. It is available for R, Python, and C
and allows fast and comfortable processing of large graphs. Many standard graph
metrics as well as an efficient graph data structure  are provided by this
library. More information can be found via \url{http://igraph.org/r/}.

\subsection{Gzoltar}
\label{sec:gzoltar}



\end{document}
