\subsection{Test Suite Characteristics}

The following metrics aim to quantitatiely analyse a project's test suite. We
use basic metrics provided via Gzoltar (see \ref{sec:gzoltar}) as well as test
suite metrics proposed by Perez et al in \enquote{A Test-suite Diagnosability
Metric for Spectrum-based Fault Localization Approaches} and shown to be good
indicators of spectrum-based fault localisation accuracy. These metrics are
computed based on the coverage data provided by Gzoltar (see \ref{sec:gzoltar}).

\subsubsection{Number of Passed Test Cases (T-NP)}

The total number of test cases out of the test suite that were passed, i.e.,
executed successfully. We measure this based on all test classes, not solely
the ones that fail for the current buggy version. This metric is calculated via
Gzoltar (see \ref{sec:gzoltar}).

\subsubsection{Proportion of Passed Test Cases (T-PP)}

The proportion of passed test cases out of all relevant test cases.

\subsubsection{Number of Failed Test Cases (T-NF)}

The total number of test cases out of the test suite that were failed, i.e.,
that could not be executed successfully. This number can be extracted by Gzoltar
(see \ref{sec:gzoltar}) or accessed via
\url{http://program-repair.org/defects4j-dissection/#!/}.

\subsubsection{Proportion of Failed Test Cases (T-PF)}

The proportion of failed test cases out of all relevant test cases. This number
can be extracted by Gzoltar (see \ref{sec:gzoltar}) or accessed via
\url{http://program-repair.org/defects4j-dissection/#!/}.

\subsubsection{Number of Test Cases (T-N)}

This metric counts the total number of test cases in the given test suite. 

\subsubsection{Number of Relevant Test Cases (T-RN)}

The number of test cases that are relevant for the buggy version, i.e., the
number of test cases in the classes in which at least one test case fails
because of the bug. This measure is provided by gzoltar (see \ref{sec:gzoltar}).

\subsubsection{Proportion of Relevant Test Cases (T-RP)}

This metric determines which proportion of all test cases in the test suite are
considered relevant, i.e., load at least one of the changed methods during
execution.

\subsubsection{Test Case Density (T-DSLOC)}

This metric measures number of test cases per 100 source lines of code, i.e., it gives an indicator of how well-tested the source code is.

\subsubsection{Test Size (T-SLOC)}

This metric indicates the size of the tests by measuring the total number of lines of code (excluding blank lines and comments) of tests.

\subsubsection{Relative Test Size (T-RSLOC)}

The relative test size quantifies the size of the test suite (measured via source lines of code) normalised by the size of the project (also in source lines of code) since we expect a strong positive correlation between these two features/

\subsubsection{Test Density (T-D)}

The test density is a measure that ensures that components are frequently
involved in tests. It is computed as T-D$=\sum{A[i,j]}/(N\cdot M)$ and then
normalised, s.t., a value of $1.0$ is the target.

\subsubsection{Test Diversity (T-G)}

The test diversity tries to quantify to what extent components are tested in
diverse combinations. We compute it as T-G$=1 - \sum{n \cdot (n-1)}/ (N \cdot
(N-1))$ which yields values in the interval $[0;1]$ where $1$ is seen as the
optimal value.

\subsubsection{Test Uniqueness (T-U)}

The test uniqueness tries to measure to what extent components are
distinguishable based on the test results. It is computed as T-U=$|\textrm{T-G}|
/ \textrm{T-M}$. 

\subsubsection{Perez et al's diagnostic predictor (T-DDU)}

This metric tries to combine the previous ones in a way that provides the
maximum amount of information concerning the test suite's influence on software
diagnosability. It is partly based on the notion of entropy. The metric is
computed via the formula $\textrm{T-DDU} = \textrm{T-P} \cdot \textrm{T-G} \cdot
\textrm{T-U}$ and, thus, produces values in the interval $[0;1]$ where a value
of $1$ is regarded as ideal.